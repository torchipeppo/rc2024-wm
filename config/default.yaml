# manifest hydra folder
hydra:
  output_subdir: hydra

device: CUDA_IF_AVAILABLE  # this default is handled in a special decorator, can be overridden
device_type: ???     # this is set in the same decorator based on the device; this should NOT be overridden

# TODO seed

dataset:
  _target_: data.MarioCSVDataset
  csv_path: v1_processed_by_egoid
  number_of_other_robots: 2  # influences transformer config
  time_span: 5               # influences transformer config

tokenizer:
  _target_: tokenizer.Tokenizer
  x_min: -4500.0
  y_min: -3000.0
  x_max: 4500.0
  y_max: 3000.0
  x_buckets: 50              # influences transformer config
  y_buckets: 30              # influences transformer config

transformer:
  # these three are set in a special decorator based on the other params, do NOT override
  vocab_size: ???      # = x_buckets * y_buckets
  num_of_blocks: ???   # = time_span
  block_size: ???      # = 2 + num_of_other_robots  (i.e. ego + others + ball)
  _target_: transformer_nanogpt.TransformerConfig
  n_layer: 6
  n_head: 12
  n_embd: 768
  dropout: 0.0
  bias: False
  attn_mask_type: block_causal

training:
  data_split: 0.95
  batch_size: 64  # partiamo molto ottimisti, vedremo poi quando ci esplode la memoria
  epochs: 100
  batches_per_epoch: 100
  #
  transformer:
    learning_rate: 6e-4
    weight_decay: 1e-1
    betas:
      - 0.9
      - 0.95

eval:
  batch_size: ${training.batch_size}
